<!DOCTYPE html>
<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->  
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->  
<!--[if !IE]><!--> <html lang="en"> <!--<![endif]-->  
<head>
    <title>SLAMBot</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">    
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'> 
    <!-- Global CSS -->
    <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css">
    <!-- Plugins CSS -->    
    <link rel="stylesheet" href="assets/plugins/font-awesome/css/font-awesome.css">
    <link rel="stylesheet" href="assets/plugins/prism/prism.css">
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="assets/css/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head> 

<body data-spy="scroll">
    
    <!---//Facebook button code-->
    <div id="fb-root"></div>
    <script>(function(d, s, id) {
      var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.0";
      fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));</script>
    
    <!-- ******HEADER****** --> 
    <header id="header" class="header">  
        <div class="container">            
            <h1 class="logo pull-left">
                <a class="scrollto" href="#promo">
                    <span class="logo-title">SLAMBot</span>
                </a>
            </h1><!--//logo-->              
            <nav id="main-nav" class="main-nav navbar-right" role="navigation">
                <div class="navbar-header">
                    <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button><!--//nav-toggle-->
                </div><!--//navbar-header-->            
                <div class="navbar-collapse collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li class="active nav-item sr-only"><a class="scrollto" href="#promo">Home</a></li>
                        <li class="nav-item"><a class="scrollto" href="#introduction">Introduction</a></li>
                        <li class="nav-item"><a class="scrollto" href="#design">Design</a></li>
                        <li class="nav-item"><a class="scrollto" href="#implementation">Implementation</a></li>
                        <li class="nav-item"><a class="scrollto" href="#results">Results</a></li>                        
                        <li class="nav-item"><a class="scrollto" href="#conclusion">Conclusion</a></li>
                        <li class="nav-item"><a class="scrollto" href="#team">Team</a></li>
                        <li class="nav-item last"><a class="scrollto" href="#materials">Additional Materials</a></li>
                    </ul><!--//nav-->
                </div><!--//navabr-collapse-->
            </nav><!--//main-nav-->
        </div>
    </header><!--//header-->
    
    <!-- ******PROMO****** -->
    <section id="promo" class="promo section offset-header">
        <div class="container text-center">
            <h2 class="title">EE125<span class="highlight"> SLAMBot</span></h2>
            <p class="intro">An AR tag-based Simultaneous Localization and Mapping robot. With style.</p>
            <div class="btns">
                <a class="btn btn-cta-primary" href="http://github.com/ugotnitined/fedora-slambot" target="_blank">Github</a>
            </div>
            <ul class="meta list-inline">
              <li>Created by: <a href='http://alexpchu.com'>Alex Chu</a>, <a href='https://github.com/JonghyunAhn'>Jong Ahn</a>, <a href='https://github.com/ugotnitined'>Nitin Sadras</a></li>
            </ul><!--//meta-->
        </div><!--//container-->
    </section><!--//promo-->
    
    <!-- ******ABOUT****** --> 
    <section id="introduction" class="about section docs">
        <div class="container">
            <div class="docs-inner">
              <center>
                <figure style='margin:2em;'>
                  <img style='width:50%;' class="img-responsive" src="assets/images/robot/front.jpg"/>
                  <figcaption>SLAMBot, the TETRIX Robot that runs our SLAM Algorithm.</figcaption>
                </figure>
              </center>
            <h2 class="title text-center">Introduction</h2>            
            <div class="block">
              <p>The objective of our project is to implement some SLAM (Simultaneous Localization and Mapping) algorithm and have it run on a mobile robot. As we drive the robot around, it will use its camera to locate a set of vision markers. The algorithm will localize these markers with respect to each other and with the robot, simultaneously estimating the map of the markers, as well as the robot's position in space.</p>
              <p>This is an interesting problem because it solves the apparent circular problem of mapping and localization. To build a map of an area based on vision features, one must have the position of the robot. To find the position of the robot in space, one must have the map of the area. The two SLAM solutions we implemented for this project cope with this by modeling the uncertainty of the robot's position and the marker positions, and selecting the map with the greatest likelihood.</p>
              <p>To make our solution work, we have to collect the data expected by SLAM, and feed it into the algorithm. The first problem we had to solve was to read the rotational and forward velocities of the robot from the gyroscope and encoders of the robot, to get a rough update on the position of the robot. The second problem we had to solve was to find a way to classify markers placed on the ground, and compute their position relative to the robot on the ground. The last problem we had to solve is to implement SLAM; given the velocities from the robot sensors and the marker classifications and positions from the vision node, compute the maximum likelihood estimation of the robot position and a map of the area.</p>
              <p>SLAM has many applications in the modern world, and is actually an active research topic in robotics. Its applications include localization and planning for autonomous vehicles, and finer grained localization for mobile devices. On one end of the spectrum, a localization algorithm that can take advantage of previously unknown landmarks and geographic features can help autonomous vehicles build a local map of their surroundings (information not available from a GPS unit), and better navigate treacherous terrain. In fact, the <a href='http://en.wikipedia.org/wiki/Stanley_%28vehicle%29'>winner</a> of the 2005 DARPA Grand Challenge, an autonomous car built by a Stanford team, used SLAM as part of its navigation algorithm. Another application of SLAM would be autonomous exploration. Perhaps a simpler application of SLAM would be better localization for mobile devices. Most mobile devices perform localization using a GPS unit, which does not work indoors, and generates position readings with a variance of about 10 meters. With SLAM, it may be possible to use the mobile device's camera, gyroscope, and accelerometer to improve these readings, taking advantage of local landmarks and features to reduce the uncertainty of the GPS reading.</p>
            </div><!--//block-->
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->

    <section id="design" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Design</h2>            
            <div class="block">
              <h3 class="sub-title text-center">Design Criteria</h3>
              SLAMBot was designed with the following criteria in mind:
              <ul>
                <li>SLAMBot must build a map, with the markers positioned correctly.</li>
                <li>The SLAM algorithm must be invariant to noise, and produce a reasonably good map, even if sensor readings are not perfect.</li>
                <li>SLAMBot must have be able to self-correct; if the robot sees a marker that it does not expect based on its map, it must either change the map or its position in the map.</li>
              </ul>
              Given these design criteria, SLAMBot should be able to produce a reasonable estimation of the marker map, even with noisy sensor readings. As we drive the robot around, it should be able to estimate its position using its gyroscope, odometer, and any markers it sees along the way.
              <h3 class="sub-title text-center">The Design</h3>
              <p></p>
              <h4 class="text-center">Hardware</h4>
              <p>We opted to develop SLAM on the provided TETRIX car platform. We did this because SLAM requires a simple mobile robot as a testing platform. Since we wanted to focus our efforts in developing our vision and localization algorithms, we decided it would be best to pick one of these pre-built kits.</p>

              <p>The Extended Kalman Filter and Particle Filter SLAM algorithms both require rotational and translational velocity inputs to estimate robot position. To provide these services, we originally intended to use a 3-axis gyroscope and accelerometer to measure rotational velocity and acceleration. We would then integrate over acceleration to get velocity. This did not end up working out very well, because error in acceleration would accumulate over time. Our final hardware solution used the encoders on the TETRIX car's motor controller as odometers, which gave us a more accurate reading on robot velocity.</p>
              <center>
                <figure style='margin:2em;'>
                  <img style='width:50%;' class="img-responsive" src="assets/images/robot/persp.jpg"/>
                  <figcaption>SLAMBot Hardware</figcaption>
                </figure>
              </center>


              <p>To provide camera support for our vision node, which would detect markers near the robot and report them to the SLAM algorithm, we had to modify the robot. Since all the markers would be flat on the ground, we angled the camera down to increase our usable visual range. This came at a cost of our ability to see objects above the horizon, but since SLAM algorithm only cares about marker positions, we decided that this cost is secondary.</p> 
              <center>
                <figure style='margin:2em;'>
                  <img style='width:50%;' class="img-responsive" src="assets/images/robot/side.jpg"/>
                  <figcaption>SLAMBot Camera</figcaption>
                </figure>
              </center>

              <h4 class="text-center">Vision</h4>
              <p>We couldn't get AR-Tags to work, so we designed and implemented our own:</p>
              <center>
                <div style='width:30%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/markers/a.png"/>
                    <figcaption>Original design for vision markers.</figcaption>
                  </figure>
                </div>
                <div style='width:10%;display:inline-block;'>
                </div>
                <div style='width:30%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/markers/5.png"/>
                    <figcaption>New design for vision markers. This one encodes a 5.</figcaption>
                  </figure>
                </div>
              </center>
              <p>Our original design for the AR-Tag consisted of four brightly colored corners, and a data pattern consisting of an alpha character. We would use homography to warp the marker to an orthographic projection, and perform normalized cross correlation with a set of library markers to get the final classification. This did not work well because we experienced alignment issues, and we did not get the desired classification accuracy. We considered various methods like contour detection and radial binning (like lab 5), but finally decided on changing the data pattern on the marker to 4 monochrome blocks, each representing a single bit. This improved our classification accuracy from about 50% to almost 100%.</p>
              <h4 class="text-center">SLAM Algorithm</h4>
              <p>Our first implementation of SLAM models the mapping and localization problem as an inference problem, and attempts to solve it using a Particle Filter. For simplicity, we make an independence assumption, estimating the position of each marker as if it were independent of the positions of all other markers. This kept our state space small, allowing us to greatly increase the number of particles we can use. We expected robustness to arise from simplicity. However, we ran into several problems with this model. First of all, the particle filter was very sensitive to noise in the sensor readings. Because of the independence assumption, the particle filter would not correct the entire map when a new marker reading becomes available. Instead, it will only relocate the affected marker, resulting in an inconsistent map. The particle filter also takes an excessive amount of time and observations to converge to a reasonable map, which would be impractical for real applications of the algorithm, such as automated exploration.</p>
              <p>=========================TODO(Jong): explain EKF design.=========================</p>
              <p></p>
            </div><!--//block-->
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->

    <section id="implementation" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Implementation</h2>            
            <div class="block">
              As stated before, SLAMBot is built on the TETRIX + Raspberry Pi platform. We chose Python and <a href='http://ros.org'>ROS</a> as our software stack, and used it to offload all of our vision and SLAM algorithm computation from the Pi to our laptops for performance purposes. As such, our project is structured as a distributed model. The robot will serve the functions of its low-level hardware as a service over ROS, and the control computer will use this data to perform high level computations like vision and SLAM. The robot will provide its sensor and camera values, and listen for commands from the driver through it's control_sub node. The controller computer will use the sensor values and raw images to do marker detection and SLAM. The output of SLAM will inform a human driver, who will provide commands to the robot through control_pub. The architecture of our project is shown below:
              <center>
                <figure style='margin:2em;'>
                  <img style='width:70%;' class="img-responsive" src="assets/images/flowchart.png"/>
                  <figcaption>The structure of SLAMBot's codebase</figcaption>
                </figure>
              </center>
              <h4 class="text-center">Hardware</h4>
              ==== TODO(Nitin) write this. Talk about how we dealt with bandwidth and vision lag, etc. Read Design first to prevent redundancy. ====
              <center>
                <div style='width:40%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/robot/gyro.jpg"/>
                    <figcaption>3-axis gyro + accelerometer</figcaption>
                  </figure>
                </div>
                <div style='width:10%;display:inline-block;'>
                </div>
                <div style='width:40%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/robot/encoder.jpg"/>
                    <figcaption>Right motor encoder</figcaption>
                  </figure>
                </div>
              </center>



              <h4 class="text-center">Vision</h4>

              <p>Our Vision pipeline is implemented using OpenCV and Numpy, with ROS as an interface to the rest of the project codebase. The vision markers consist of a binary data pattern in the center of a sheet, with four colored alignment squares. The Vision node reads image data from /throttled_camera_image, and outputs marker classifications and positions to /markers.</p>

              <center>
                <div style='width:20%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/vision/raw.png"/>
                    <figcaption>Raw Image</figcaption>
                  </figure>
                </div>
                <div style='width:20%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/vision/blobs.png"/>
                    <figcaption>Blob Detection</figcaption>
                  </figure>
                </div>
                <div style='width:20%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/vision/detect.png"/>
                    <figcaption>Marker Detection</figcaption>
                  </figure>
                </div>
                <div style='width:20%;display:inline-block;'>
                  <figure>
                    <img class="img-responsive" src="assets/images/vision/rectify.png"/>
                    <figcaption>Rectified Marker</figcaption>
                  </figure>
                </div>
              </center>

              <p>The vision pipeline takes an input image and does HSV Filtering to find probable marker corners. It looks for patches of image that are sufficiently red, green, blue, or yellow, and uses OpenCV's morphologyEx functions to eliminate small patches. We then use OpenCV's findContours to detect blobs, and keep the blobs that are above a certain area threshold, performing some measure of non-maximum suppression. Now that we have a list of red, green, blue, and yellow blobs, we can look for markers. Using a homography computed at camera calibration, we now build a graph of potential successors for each blob. A successor for a red blob, for instance, would be a green blob at least 8.5 inches away. A successor for a green blob is a blue blob 11 inches away, and so on. We perform cycle detection on the graph. Any cycle of length four constitutes a marker. Now that an oriented set of four corners for each marker, we can do marker classification. We compute a homography matrix using the corners, and use <a href='http://blogs.mathworks.com/steve/2006/05/05/spatial-transformations-inverse-mapping/'>inverse warping</a> to rectify and extract the pattern of the marker. Now, we simply bin the marker into four quadrants, and output the classification. The above marker was classified as a 10, since its second and fourth bits are true.</p>

              <h4 class="text-center">SLAM Algorithm</h4>
              ======TODO(Jong): Write this======

            </div><!--//block-->
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->

 
    <section id="results" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Results</h2>            
            <div class="block">
            </div><!--//block-->
                <center>
                    <iframe width="650" height="400" src="//www.youtube.com/embed/wASgk7cUkuI" frameborder="0" allowfullscreen></iframe>
                </center>
                <br>
                <p>
                    Here, you can see our robot mapping out two marker configurations - circular and linear.  The top right screen shows a visualization of the particle-filter algorithm, while the bottom left screen shows the Extended Kalman filter Slam algorithm.  The bottom right screen shows the view from the robot's on-board camera, as well as the homography image that is used for marker classification. It is clear from the algorithm visualizations that the EKF both converges more quickly and constructs a more accurate map than the particle filter. 
                </p>

                <p>
                    As seen in the video, our robot was very succesful in mapping and localizing itself with respect to small, closely packed marker configurations.  In such maps, even if the robot is picked up and place somewhere else, it succesfully corrects the map after making a sufficient number of observations.  However, when we attempted to map out larger, more spread out configurations, the relative distances between marker ane the robot's position were off.  This is likely due to sensor drift over time, as well as error accumulating from our velocity calculations.
                </p>
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->
 
    <section id="conclusion" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Conclusion</h2>            
            <div class="block">
            </div><!--//block-->
            =====TODO(somebody): Discuss your results. How well did your finished solution meet your design criteria?====
            =====TODO(somebody): Did you encounter any particular difficulties?====
            =====TODO(somebody): Does your solution have any flaws or hacks? What improvements would you make if you had additional time?====
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->
 
    <section id="team" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Meet the Team</h2>            
            <div class="block">
              <h4 class="text-center">Alexander Chu</h4>
            =====TODO(Alex): Write Bio. Add photo.====
              <h4 class="text-center">Jonghyun Ahn</h4>
            =====TODO(Jong): Write Bio. Add photo.====
              <h4 class="text-center">Nitin Sadras</h4>
            =====TODO(Nitin): Write Bio. Add photo.====
            </div><!--//block-->
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->
 
    <section id="materials" class="about section docs">
        <div class="container">
            <div class="docs-inner">
            <h2 class="title text-center">Additional Materials</h2>            
            <div class="block">
              =====TODO(somebody):Code, URDFs, and launch files you wrote=====
              =====TODO(somebody):CAD models for any hardware you designed=====
              =====TODO(somebody):Datasheets for components used in your solution=====
              =====TODO(somebody):Any additional videos, images, or data from your finished solution=====
            </div><!--//block-->
            </div><!--//docs-inner-->         
        </div><!--//container-->
    </section><!--//about-->
 
    
    <section id="contact" class="contact section has-pattern">
    </section><!--//contact-->  
      
    <!-- ******FOOTER****** --> 
    <footer class="footer">
        <div class="container text-center">
            <small class="copyright">Designed with <i class="fa fa-heart"></i> by <a href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small>
        </div><!--//container-->
    </footer><!--//footer-->
     
    <!-- Javascript -->          
    <script type="text/javascript" src="assets/plugins/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery-migrate-1.2.1.min.js"></script>    
    <script type="text/javascript" src="assets/plugins/jquery.easing.1.3.js"></script>   
    <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>     
    <script type="text/javascript" src="assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script> 
    <script type="text/javascript" src="assets/plugins/prism/prism.js"></script>    
    <script type="text/javascript" src="assets/js/main.js"></script>       
</body>
</html> 

